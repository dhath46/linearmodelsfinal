---
title: "Linear Models for Data Science Final Project"
author: "Christine George"
date: "7/24/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r}
# Read in data
setwd("C:/Users/chris/Documents/UVA/STAT_6021/FP")
DoctorContacts <- read.csv("DoctorContacts.csv")
```

```{r}
# Load packages
library(tidyverse)
library(broom)
library(ISLR2)
library(glmnet)
```

```{r}
# Rename columns
DoctorContacts <- DoctorContacts %>%
  rename(visits = mdu, log_coinsurance = lc, log_api = lpi,
         log_max_deductible = fmde, num_disease = ndisease,
         log_income = linc, log_fam_size = lfam, schooling= educdec)
```

```{r}
# Delete extra index column
DoctorContacts <- select(DoctorContacts, -rownames) 
```

```{r}
# Undo log transformations
DoctorContacts$insurance = exp(DoctorContacts$log_coinsurance)
DoctorContacts$payment = exp(DoctorContacts$log_api)
DoctorContacts$deductible = exp(DoctorContacts$log_max_deductible)
DoctorContacts$income = exp(DoctorContacts$log_income)
DoctorContacts$family = exp(DoctorContacts$log_fam_size)
```

```{r}
# Assumptions
# Model and preds with all predictors
model1 <- lm(visits~. - visits,
             data=DoctorContacts)
coef(model1)

DCs_pred <- mutate(DoctorContacts, predictions=fitted(model1),
                        resid=residuals(model1))

# Independent variances
ggplot(DCs_pred, aes(x=predictions, y=resid)) + 
  geom_point() +
  geom_hline(yintercept = 0, color="red")
# Woah, this assumption does not appear to be met.

# Normally distributed variances
ggplot(DCs_pred, aes(sample=resid)) +
  stat_qq() +
  stat_qq_line()
# Looks like we have quite a few extreme observations on the upper end.
```
```{r}
# Two sample t test to investigate mean doctor visits among women and men
# First, subset males and females
M_visits <- subset(DoctorContacts, sex == "male",
select=c(visits, sex))

F_visits <- subset(DoctorContacts, sex == "female",
select=c(visits, sex))
```

```{r}
t.test(M_visits$visits, F_visits$visits, alternative = "two.sided", mu = 0, paired = FALSE, conf.level = 0.95)
# H0: The mean number of doctor visits for men and women are equal
# Ha: The mean number of doctor visits for men and women are not equal

# Since the p value is 2.2e-16, which less than 0.05, we reject the null hypothesis. We have sufficient evidence to conclude that the mean number of doctor visits for men and women are not equal.

# Since both the lower and upper limits of the confidence interval are negative, we can conclude that on average, men have between approximately 0.707 and 0.953 less doctor visits than women.
```

```{r}
# Really wanted to double check since that result surprised me.
mean(F_visits$visits)
```

```{r}
mean(M_visits$visits)
```
#### Boxplot for Kayla ####

```{r}
DoctorContacts$insurance <- round(DoctorContacts$insurance, 2)

# unique(DoctorContacts$insurance)

ggplot(DoctorContacts, aes(x = as.factor(insurance), y = num_disease, fill = as.factor(insurance))) + 
  geom_boxplot() +
  labs(x = "Insurance Rate", y = "Number of Diseases")
```

#### Devon's code ####

Logistic Regression
```{r}
drs<-read.csv('DrContactsNew.csv')
View(drs)
```

Transform health column into binary as column 'condition'
Used log data from original data set for this
```{r}
 drs1<- DoctorContacts %>%
  mutate(condition = case_when(
    health %in% c("poor", "fair") ~ "unhealthy",
    health %in% c("good", "excellent") ~ "healthy",
    TRUE ~ as.character(health)
  ))

# View(drs1)
```

Bar graph shows relationship between condition and sex. (Just for fun)
```{r}
ggplot(drs1, aes(x=sex, fill=factor(condition), color=factor(condition)))+
  geom_bar(position='fill')
```


Make all categoricals factors (0,1)
```{r}
drs2<-drs1[, -c(1,9)]%>%
  mutate(physlim=ifelse(physlim=="TRUE", 1,0),
         idp=ifelse(idp=="TRUE", 1,0),
         sex=ifelse(sex=="male", 1,0),
         child=ifelse(child=="TRUE", 1,0),
         black=ifelse(black=="TRUE", 1,0),
         condition=ifelse(condition=="healthy", 1,0))
# View(drs2)
```

Remove response/unneeded variables, make correlation matrix
```{r}
dat<-drs2[, -15]
cor_mat<-round(cor(dat),2)
ggcorrplot::ggcorrplot(cor_mat, lab=T, type='lower')
```
There is multicollinearity between age and child (to be expected) and
log_max_deductible and log_coinsurance (also to be expected). I will remove
log_max_deductible and child.

```{r}
dat2<-dat[, -c(5,13)]
cor_mat2<-round(cor(dat2),2)
ggcorrplot::ggcorrplot(cor_mat2, lab=T, type='lower')
```

```{r}
drs3<-drs2[, -c(5,13)]
```

Create logistic regression model using glm
```{r}
logit_model1<-glm(condition~., drs3, family='binomial')
summary(logit_model1)
```
It looks like family size, coinsurance, idp, and sex are not signficant,
so I will remove them in the next iteration.

```{r}
logit_model2<-glm(condition~.-idp-log_coinsurance-log_fam_size-sex, drs3, family='binomial')
summary(logit_model2)
```
Yay! Check with AIC
```{r}
aic<-MASS::stepAIC(logit_model1, direction='both', trace=F)
summary(aic)
```
Matches logit_model2, yay!

```{r}
car::vif(logit_model2)
```
Low VIF suggests no multicollinearity (also shown by correlation matrix?) Yay!

```{r}
# Not sure why AIC for logit_model2 is higher than logit_model1
# using summary() to identify statistically significant predictors is called the Wald test.

# Trying LASSO 

x <- model.matrix(condition ~ .-condition, drs3)[,-1]
y <- drs3$condition

# Fit the model
lasso_model <- cv.glmnet(x, y, family = "binomial", alpha = 1)

# Extract coefficients
coef(lasso_model, s = "lambda.min")
# This shrank log_coinsurance to zero
```

```{r}
# Predicted probability of healthy self-rating (1 = healthy, 0 = unhealthy)

# Something to consider in limitations: how come people can have 4.3 or 1.5 diseases?

# Remove physlim

# predict(logit_model2, data.frame(visits = 2, physlim = 0, num_disease = 4, log_income = 10, schooling = 12, age = 35,
# black = 0, payment = 226, deductible = 2120, income = 2500,
# family = 8), 
#         type="response")
```









